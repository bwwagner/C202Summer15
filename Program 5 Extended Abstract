Programming Assignment V
Extended Abstract
Branden Wagner, CSCI-C202
07/29/15

	Binary trees are a powerful data structure which organizes data in a “tree.” This means that each element of the structure has data and two references to other elements, also known as a nodes. We tested the performance of a binary tree structure in the common task of spellchecking, which requires that random nodes in the tree be accessed. We found that the binary search tree structure was very efficient at this task when properly structured.  Two plaintext files were used, the first was a dictionary and the second was the text to be checked.
	The dictionary text file contained 134,173 lines with one word each, in a random order.  The second file, Oliver.txt, was a copy of The Project Gutenberg EBook of The Complete PG Works of O. W. Holmes, Sr. This file was approximately 992,130 words in length and about 5.5 MB in size.  Linear comparisons between the two files could result in thousands of comparisons, the majority of which would be irrelevant, opposed to a binary search algorithm which would make an estimated average of 9 comparisons.
	We created a custom binary tree class which included all methods we needed to perform this test. In a binary search tree nodes are arranged such that the left reference is smaller and the right reference points to a larger data element, if one exists. 
	The search algorithm we implemented traversed a tree using the natural organization of a binary search tree to cut down comparisons.  The performance of the tree structure is heavily dependent on the total amount of nodes to the left and to the right of the first node.  Another factor is the overall depth of the tree, however this can be addressed by splitting up the trees into smaller sets.  We organized our data so there were 26 trees which only contained words beginning with a single letter of the alphabet. 
	The input file contained many non-word characters which had to be parsed while forming strings.  The algorithm used to separate these characters read the file as single characters and concatenated when it encountered a non-alphabetic character.  Both the dictionary and input words were shifted to all lower case to reduce the variance between characters.
	We found 54,648 misspelled words and 937,492 correctly spelled words while comparing our random dictionary to Oliver.txt.  There were an average of 15.3 comparisons per word found, and an average of 10.4 comparisons to determine a misspelled word. The comparison algorithm took an average of 562466766.2 nanoseconds runtime to search the input file.
	One area where the spell checking accuracy could be improved is by implementing an edit distance comparison, such as Levenshtein distance.  This would allow more words to be correctly matched for difference in tense, however it could decrease overall program speed.


