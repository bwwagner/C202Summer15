Programming Assignment IV
Extended Abstract
Branden Wagner, CSCI-C202
07/12/15

  In Java, a linked list is a sequential collection of objects where each object has a reference to the next object in the list.  This data structure is very useful, but it does have limitations.  We tested the performance of a linked list structure in the common task of spellchecking, which requires that random elements in the list be accessed. We found that the linked list was significantly less efficient at this task than a binary search of an array. Two plaintext files were used, the first was a dictionary and the second was the text to be checked.
  The dictionary text file contained 134,173 lines with one word each, in a random order.  The second file, Oliver.txt, was a copy of The Project Gutenberg EBook of The Complete PG Works of O. W. Holmes, Sr. This file was approximately 992,130 words in length and about 5.5 MB in size.  Linear comparisons between the two files could result in thousands of comparisons, the majority of which would be irrelevant, opposed to a binary search algorithm which would make an average of 9 comparisons.
  The search algorithm we implemented was a linear search of the linked list, due to the limitations of the data structure. The single reference nodes that compose a linked list do not lend themselves to a binary search as each node can only compare itself to its neighbors.  This means that even if we were to use indices to search the array, we are still limited by the need to linearly search through the list to find that position. 
  The java linked list was tested for baseline performance, however, the encapsulation of this standard class meant that we could not thoroughly test the number of comparisons made during the spellchecking.  We created a custom linked list class which included all methods we needed to perform this test.  Both structures returned the same word counts and ran at an identical timeframe.
	The input file contained many non-word strings which had to be parsed.  The algorithm used to separate these characters read the file as single characters and concatenated when it encountered a non-alphabetic character.  Both the dictionary and input words were shifted to all lower case to reduce the variance between characters, and then we created several linked lists which only contained words that started with the same alphabetic character.
  We found 54,656 misspelled words and 937,484 correctly spelled words in Oliver.txt.  It took an average of 7,381 comparisons to determine a misspelled word and an average of 3,559 comparisons to verify a correctly spelled word. The total runtime for the program was about 20 seconds. This performance is typical of linked lists, which have a linear search time complexity, O(n).
  One area where the spelling accuracy could be improved is by implementing an edit distance comparison, such as Levenshtein distance.  This would allow more words to be correctly matched for difference in tense, however it could decrease overall program speed.
